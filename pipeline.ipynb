{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gltut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gltut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gltut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# HTML scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# String manipulation - from string to vector\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Neural network requirements\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import HTML file and splitting it in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gltut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to get HTML content and tokenize into sentences\n",
    "def fetch_html_and_tokenize(url):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # Remove newline characters from each sentence\n",
    "    sentences = [sentence.replace('\\n', '') for sentence in sentences]\n",
    "    df = pd.DataFrame(data = sentences, columns = ['text'])\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example URL\n",
    "url = r'https://www.trendmicro.com/en_us/research/19/b/trickbot-adds-remote-application-credential-grabbing-capabilities-to-its-repertoire.html'\n",
    "df = fetch_html_and_tokenize(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384e7bb385524daabbc2e7fc224989c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize (batch):\n",
    "    return tokenizer(batch[\"text\"], max_length=512, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Convert the DataFrame to a datasets.Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Create a DatasetDict with a single dataset\n",
    "dataset_dict = DatasetDict({'my_dataset': dataset})\n",
    "ds_encoded = dataset_dict.map(tokenize, batched= True, batch_size= None)\n",
    "\n",
    "# Tokenization\n",
    "X = [tokenizer(text, padding=\"max_length\",max_length = 512, truncation=True)['input_ids'] for text in ds_encoded[\"my_dataset\"]['text']]\n",
    "X = np.array(X, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "my_reloaded_model = tf.keras.models.load_model(\n",
    "       'Detector/output/detector-bert.keras',\n",
    "       custom_objects={\"TFBertModel\": transformers.TFBertModel}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 39s 14s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>detection</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trickbot Adds Credential-Grabbing Capabilities...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.7257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Respond Faster.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>See More.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Respond Faster.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Move faster than your adversaries with powerfu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  detection  confidence\n",
       "0  Trickbot Adds Credential-Grabbing Capabilities...      False      0.7257\n",
       "1                                    Respond Faster.       True      0.6820\n",
       "2                                          See More.       True      0.7959\n",
       "3                                    Respond Faster.       True      0.6820\n",
       "4  Move faster than your adversaries with powerfu...       True      0.6758"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confidence_col(num : float):\n",
    "    if num <= 0.5:\n",
    "        return np.round(1 - num, 4)\n",
    "    else:\n",
    "        return np.round(num, 4)\n",
    "\n",
    "def detection_col(num : float):\n",
    "    return bool(np.round(num, 0))\n",
    "\n",
    "\n",
    "prediction_sample= my_reloaded_model.predict(X)\n",
    "\n",
    "# Building the final detection dataset\n",
    "df['prediction'] = prediction_sample\n",
    "df['detection'] = df['prediction'].apply(detection_col)\n",
    "df['confidence'] = df['prediction'].apply(confidence_col)\n",
    "df = df.drop('prediction', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 512, 100)          934200    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512, 100)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200)               240800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 190)               38190     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,213,190\n",
      "Trainable params: 1,213,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_path = r'Classifier\\output\\classifier.keras'\n",
    "classifier = tf.keras.models.load_model(classifier_path)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "s_words = stopwords.words('english')\n",
    "\n",
    "def cleaning(row):\n",
    "    row = re.sub(r'http\\S+', '', row)\n",
    "    row = re.sub(\"[^a-zA-Z0-9]\", \" \", row)\n",
    "    row = nltk.word_tokenize(row.lower())\n",
    "    row = [lemma.lemmatize(word) for word in row]\n",
    "    row = [word for word in row if word not in s_words]\n",
    "    row = \" \".join(row)\n",
    "    return row\n",
    "\n",
    "df['MB'] = df['text'].apply(cleaning)\n",
    "            \n",
    "voc_size = 9000\n",
    "max_sent_length = 512\n",
    "\n",
    "one_hot = [one_hot(words, voc_size) for words in df['MB']]\n",
    "pad= pad_sequences(one_hot, padding = 'pre', maxlen = max_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gltut\\AppData\\Local\\Temp\\ipykernel_32168\\2684924810.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['classification'][index] = attack_ids\n",
      "C:\\Users\\gltut\\AppData\\Local\\Temp\\ipykernel_32168\\2684924810.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['classification_conf'][index] = probabilities\n"
     ]
    }
   ],
   "source": [
    "encoder = joblib.load(r'Classifier\\label_encoder.pkl')\n",
    "\n",
    "# Build the dataset with the classification of the sentences in MITRE\n",
    "\n",
    "_ = [[] for i in range(df.shape[0])]\n",
    "df['classification'], df['classification_conf'] = _, _\n",
    "prediction_sample= classifier(pad)\n",
    "predictions_confidences = []\n",
    "prob_tensor = tf.keras.activations.softmax(prediction_sample, axis = -1)\n",
    "\n",
    "\n",
    "for index in range(df.shape[0]):\n",
    "        \n",
    "        if df['detection'][index]:\n",
    "            top_k_probabilities, top_k_classes = tf.math.top_k(prob_tensor[index], k=5)\n",
    "            attack_ids=[]\n",
    "            probabilities=[]\n",
    "\n",
    "            for classy in top_k_classes:\n",
    "                attack_ids.append(str(encoder.inverse_transform([classy])[0]))\n",
    "\n",
    "            for probability in top_k_probabilities:\n",
    "                probabilities.append(np.round(float(probability), 4))\n",
    "\n",
    "            df['classification'][index] = attack_ids\n",
    "            df['classification_conf'][index] = probabilities\n",
    "    \n",
    "df_result = df.drop('MB', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
